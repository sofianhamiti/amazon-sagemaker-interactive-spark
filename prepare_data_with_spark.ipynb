{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive data process with PySpark on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstrates how to process data interactively with PySpark and Amazon SageMaker.\n",
    "\n",
    "There are many ways to do this and here are 3 options you can start with:\n",
    "\n",
    "* Option A: On a **Notebook Instance** to develop locally with Spark. (single node)\n",
    "* Option B: With **SageMaker Studio** and its native integration to [Glue Interactive sessions](https://aws.amazon.com/blogs/machine-learning/prepare-data-at-scale-in-amazon-sagemaker-studio-using-serverless-aws-glue-interactive-sessions). (serverless)\n",
    "* Option C: With **SageMaker Studio** and its native integration to [EMR clusters](https://catalog.workshops.aws/sagemaker-studio-emr/en-US/01-interacting-emr-cluster). (cluster-based)\n",
    "\n",
    "\n",
    "This Notebook shows the local Spark processing you can run in your **Notebook Instance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/sm_spark.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option A: you can use Spark on your Notebook Instance (single node) \n",
    "Develop and debug Spark code quickly, on a smaller or sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# get notebook instance IAM role\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# get temporary credentials from IAM role, so Spark can use them to read/write to S3\n",
    "sts = boto3.client('sts')\n",
    "response = sts.assume_role(RoleArn=iam_role, RoleSessionName='pyspark',DurationSeconds=3600)\n",
    "credentials = response['Credentials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark Session \n",
    "We add [S3A dependencies](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html) to the session, so Spark can talk to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2,com.amazonaws:aws-java-sdk-bundle:1.11.888\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", credentials['AccessKeyId'])\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", credentials['SecretAccessKey'])\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", credentials['SessionToken'])\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA FROM S3\n",
    "df = spark.read.csv(\"<ADD YOUR S3A FILE INPUT HERE>\", header=True) # example: s3a://mybucket/spark-input/dataset.csv\n",
    "\n",
    "# ==================================================\n",
    "# ============= DO PROCESSING HERE =================\n",
    "# ==================================================\n",
    "\n",
    "# UPLOAD PROCESSED DATA TO S3\n",
    "df.write.parquet(\"<ADD YOUR S3A FILE OUTPUT HERE>\", mode=\"overwrite\") # example: s3a://mybucket/spark-output/dataset.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option B and C are with SageMaker Studio\n",
    "Go to SageMaker Studio, and follow instructions for [Glue integration](https://aws.amazon.com/blogs/machine-learning/prepare-data-at-scale-in-amazon-sagemaker-studio-using-serverless-aws-glue-interactive-sessions) or [EMR integration](https://catalog.workshops.aws/sagemaker-studio-emr/en-US/01-interacting-emr-cluster)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
